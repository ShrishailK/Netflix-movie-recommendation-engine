{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8b4191",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60404fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary pacakages\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import scipy.sparse\n",
    "from numpy import savez_compressed\n",
    "from numpy import load\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881074ee",
   "metadata": {},
   "source": [
    "# Result visualization for IDF,TF-IDF, Bag of words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657dc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function for results \n",
    "def display_img(url):\n",
    "    #Get url of the product and download it\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    plt.imshow(img)\n",
    "\n",
    "def heatmap_image(keys,values,labels,url,text):\n",
    "    #keys gives the list of words for recommended title\n",
    "    #divide the figure into two parts\n",
    "    \n",
    "    gs = gridspec.GridSpec(1,2,width_ratios = [4,1])\n",
    "    fg = plt.figure(figsize=(25,3))\n",
    "    \n",
    "    #1st figure plotting a heatmap that represents the most commonly occuring words\n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax = sns.heatmap(np.array([values]),annot=np.array([labels]))\n",
    "    ax.set_xticklabels(keys)\n",
    "    ax.set_title(text)                 \n",
    "    \n",
    "    #2nd figure plotting a heatmap that represents the image of the product\n",
    "    ln = plt.subplot(gs[1])\n",
    "    ln.set_xticks([])\n",
    "    ln.set_yticks([])\n",
    "    \n",
    "    fig = display_img(url)\n",
    "    \n",
    "    #display combine figure\n",
    "    plt.show()\n",
    "\n",
    "def heatmap_image_plot(doc_id,vec1,vec2,url,text,model,tfidf_title_vectorizer,tfidf_title_features,idf_title_vectorizer,idf_title_features):\n",
    "    \n",
    "                     \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    \n",
    "    #set the value of non intersecting word to zero in vec2                 \n",
    "    for i in vec2.keys():\n",
    "        if i not in intersection:\n",
    "            vec2[i]=0\n",
    "    #if ith word in intersection(list of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0                 \n",
    "    values = [vec2[x] for x in vec2.keys()]\n",
    "    \n",
    "    #labels for heatmap\n",
    "    keys = list(vec2.keys())\n",
    "                     \n",
    "    if model == 'bag_of_words':\n",
    "        labels = values\n",
    "    \n",
    "    elif model == 'Tfidf':\n",
    "        labels = []\n",
    "        for i in vec2.keys():\n",
    "            if i in tfidf_title_vectorizer.vocabulary_:\n",
    "                #idf_title_vectorizer.vocabulary contains all the words in the corpus         \n",
    "                labels.append(tfidf_title_features[doc_id,tfidf_title_vectorizer.vocabulary_[i]])\n",
    "        \n",
    "            else:\n",
    "                labels.append(0)\n",
    "    elif model == 'idf':\n",
    "        labels = []\n",
    "        for i in vec2.keys():\n",
    "            if i in idf_title_vectorizer.vocabulary_:\n",
    "                #idf_title_vectorizer.vocabulary contains all the words in the corpus         \n",
    "                labels.append(idf_title_features[doc_id,idf_title_vectorizer.vocabulary_[i]])\n",
    "        \n",
    "            else:\n",
    "                labels.append(0)\n",
    "                     \n",
    "    heatmap_image(keys,values,labels,url,text)\n",
    "                     \n",
    "                     \n",
    "def text_vector(sentence):\n",
    "    words = sentence.split()    \n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "def results(doc_id,sentence1,sentence2,url,model,tfidf_title_vectorizer,tfidf_title_features,idf_title_vectorizer,idf_title_features):\n",
    "    vec1 = text_vector(sentence1)\n",
    "    vec2 = text_vector(sentence2)\n",
    "                     \n",
    "    heatmap_image_plot(doc_id,vec1,vec2,url,sentence2,model,tfidf_title_vectorizer,tfidf_title_features,idf_title_vectorizer,idf_title_features)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20e712",
   "metadata": {},
   "source": [
    "# Result visualization for Avg Word2Vec amd Weighted Word2Vec Vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d59561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function to better visualize and understand results\n",
    "\n",
    "def get_word_vec(sentence,doc_id):\n",
    "    #doc_id : index id in vectorized array\n",
    "    #sentence : title of product\n",
    "    #model_name : 'avg', we will append the model[i], w2v representation of word i\n",
    "    \n",
    "    vec = []\n",
    "    for i in sentence.split():\n",
    "        if i in vocab:\n",
    "                vec.append(modl[i])\n",
    "        else:\n",
    "            vec.append(np.zeros(shape=(300,)))\n",
    "    return np.array(vec)\n",
    "def get_distance(vec1,vec2):\n",
    "    # vec1 = np.array(#number_of_words_title1 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    # vec2 = np.array(#number_of_words_title2 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    final_dist = []\n",
    "    for i in vec1:\n",
    "        dist = []\n",
    "        for j in vec2:\n",
    "            dist.append(np.linalg.norm(i-j))\n",
    "        final_dist.append(np.array(dist))\n",
    "            \n",
    "    return np.array(final_dist)\n",
    "\n",
    "def results_Word2Vec(sentence1,sentence2,url,doc_id1,doc_id2):\n",
    "    # sentence1 : title1, input product\n",
    "    # sentence2 : title2, recommended product\n",
    "    # :  'avg'\n",
    "\n",
    "    sentence_vec1 = get_word_vec(sentence1,doc_id1)\n",
    "    sentence_vec2 = get_word_vec(sentence2,doc_id2)\n",
    "    \n",
    "    #sent1_sent2_dist = eucledian distance between i and j\n",
    "    #sent1_sent2_dist = np array with dimensions(#number of words in title1 * #number of words in title2)\n",
    "    sent1_sent2_dist = get_distance(sentence_vec1,sentence_vec2)\n",
    "    \n",
    "    # devide whole figure into 2 parts 1st part displays heatmap 2nd part displays image of products\n",
    "    \n",
    "    gs = gridspec.GridSpec(1,2,width_ratios=[4,1])\n",
    "    fg = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax = sns.heatmap(np.round(sent1_sent2_dist,3), annot = True)\n",
    "    ax.set_xticklabels(sentence2.split())\n",
    "    # set the y axis labels as input apparels title\n",
    "    ax.set_yticklabels(sentence1.split())\n",
    "    # set title as recommended apparels title\n",
    "    ax.set_title(sentence2)\n",
    "    \n",
    "    #setting the fontsize and rotation of x tick tables\n",
    "    ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 12,rotation=90)\n",
    "    ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 12,rotation=45)\n",
    "    \n",
    "    fg = plt.subplot(gs[1])\n",
    "    fg.set_xticks([])\n",
    "    fg.set_yticks([])\n",
    "    fig = display_img(url)\n",
    "    \n",
    "    #display combine figure\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd4180",
   "metadata": {},
   "source": [
    "# Functions for Vectorization :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e83c7",
   "metadata": {},
   "source": [
    "#### Function for IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions for IDF\n",
    "def containing(word,df):\n",
    "    #returns the number of documents which have the word\n",
    "    return sum(1 for sentence in df['title'] if word in sentence.split())\n",
    "def idf(word,df):\n",
    "    #return the idf value for a word\n",
    "    return math.log(df.shape[0]/(containing(word,df)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17ff1d",
   "metadata": {},
   "source": [
    "####  Function for Word2Vec vectorization\n",
    "We perform Word2Vec vectorization in advance to use the vectorized array directly in distance based similarity recommendation as Word2Vec vectorization in computationally intensive as compared bag of words and tfidf vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e835cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_vec(sentence,no_features):\n",
    "    \n",
    "    # sentence: title of the apparel\n",
    "    # num_features: the lenght of word2vec vector, its values = 300\n",
    "    # model_name: model information\n",
    "    \n",
    "    featureVec = np.zeros(shape=(300,), dtype=\"float32\")\n",
    "    # intialize a vector of size 300 with all zeros\n",
    "    # add each word2vec(wordi) to this fetureVec\n",
    "\n",
    "    ncount = 0\n",
    "    for word in sentence.split():\n",
    "        ncount += 1\n",
    "        if word in vocab:\n",
    "                featureVec = np.add(featureVec,modl[word])\n",
    "            \n",
    "        if (ncount>0):\n",
    "            featureVec = np.divide(featureVec,ncount)\n",
    "\n",
    "    #return avg vec\n",
    "    return featureVec    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d564f5",
   "metadata": {},
   "source": [
    "#### Downloading the google Word2Vec library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading Googles Word2Vec library to be used in all word to vec models\n",
    "#using a pretrained model by google\n",
    "#download \"GoogleNews-vectors-negative300.bin\" \n",
    "\n",
    "modl = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#vocab = stores all the words in google Word2vec model\n",
    "vocab = modl.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069eaa43",
   "metadata": {},
   "source": [
    "# Vectorization : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorization(data,model):\n",
    "    #data : Data set containing text data\n",
    "    #model : method used for text vectorization\n",
    "    \n",
    "    if model == 'bag_of_words':\n",
    "        #Vectorization using Bag of words\n",
    "        title_vectorizer = CountVectorizer()\n",
    "        title_features = title_vectorizer.fit_transform(data['title'])\n",
    "        \n",
    "        return title_features,title_vectorizer\n",
    "    \n",
    "    elif model == 'Tfidf':\n",
    "        #Vectorization using tfidfVectorizer\n",
    "        tfidf_title_vectorizer = TfidfVectorizer()\n",
    "        tfidf_title_features = tfidf_title_vectorizer.fit_transform(data['title'])\n",
    "        \n",
    "        return tfidf_title_features,tfidf_title_vectorizer\n",
    "    \n",
    "    elif model == 'idf':\n",
    "        #Vectorization using idf function\n",
    "        idf_title_vectorizer = CountVectorizer()\n",
    "        idf_title_features = idf_title_vectorizer.fit_transform(data['title'])\n",
    "        # idf_title_features.shape = #no of_data_points * #words_corpus\n",
    "        # CountVectorizer().fit_transform(courpus) returns the a sparase matrix of dimensions #data_points * #words_in_corpus\n",
    "        \n",
    "        #converting all the values into float\n",
    "        idf_title_features = idf_title_features.astype(np.float)\n",
    "        \n",
    "        #assigning df value for idf[value] function\n",
    "        df = data\n",
    "        \n",
    "        for i in idf_title_vectorizer.vocabulary_.keys():\n",
    "            idf_value = idf(i,df)\n",
    "            #j is the index of the nonzero values\n",
    "            for j in idf_title_features[:,idf_title_vectorizer.vocabulary_[i]].nonzero()[0]:\n",
    "                idf_title_features[j,idf_title_vectorizer.vocabulary_[i]] = idf_value\n",
    "        \n",
    "        scipy.sparse.save_npz('Pickle/idf_title_features.npz', idf_title_features)\n",
    "        \n",
    "        return idf_title_features,idf_title_vectorizer\n",
    "    \n",
    "    elif model == 'avg':\n",
    "        w2vec_title_features = []\n",
    "        #building vector for each title \n",
    "        for i in data['title']:\n",
    "            w2vec_title_features.append(avg_word_vec(i,300))\n",
    "\n",
    "        #w2v_title_features = np.array(# number of doc/rows in courpus * 300) \n",
    "        Word2Vec_features = np.array(w2vec_title_features)\n",
    "        \n",
    "        #saving dataframe in a npz file\n",
    "        savez_compressed(\"Pickle/Word2Vec_avg.npz\",Word2Vec_features)\n",
    "        \n",
    "        return w2vec_title_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
