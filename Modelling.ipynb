{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8b4191",
   "metadata": {},
   "source": [
    "# Importing Libraries and Storing Googles Word2vec libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60404fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary pacakages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import scipy.sparse\n",
    "from numpy import savez_compressed\n",
    "from numpy import load\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import math\n",
    "from ipynb.fs.full.Visualization import *\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "#Downloading Googles Word2Vec library to be used in all word to vec models using a pretrained model by google\n",
    "#download \"GoogleNews-vectors-negative300.bin\" \n",
    "modl = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#vocab = stores all the words in google Word2vec model\n",
    "vocab = modl.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd4180",
   "metadata": {},
   "source": [
    "# Result Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8b0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class results():\n",
    "    \n",
    "    def __init__(self,doc_id,model,cut_off):        \n",
    "        \n",
    "        #initializing the movie for recommendation\n",
    "        self.doc_id = doc_id\n",
    "        \n",
    "        #initialzing the model to be used\n",
    "        self.model = model\n",
    "        \n",
    "        #initialzing the data to be modelled\n",
    "        self.data = pd.read_pickle('Pickle/preprocessed_data')\n",
    "        \n",
    "        #assigning data frame used to display results\n",
    "        self.df = pd.read_pickle('Pickle/original_data')\n",
    "        \n",
    "        #the number of recommendations we require\n",
    "        self.cut_off = cut_off\n",
    "        \n",
    "    def Vectorization(self):\n",
    "        #data : Data set containing text data\n",
    "        #model : method used for text vectorization\n",
    "\n",
    "        if self.model == 'bag_of_words':\n",
    "            #Vectorization using Bag of words\n",
    "            title_vectorizer = CountVectorizer()\n",
    "            title_features = title_vectorizer.fit_transform(self.data['text'])   \n",
    "            return title_features,title_vectorizer\n",
    "\n",
    "        elif self.model == 'Tfidf':\n",
    "            #Vectorization using tfidfVectorizer\n",
    "            tfidf_title_vectorizer = TfidfVectorizer()\n",
    "            tfidf_title_features = tfidf_title_vectorizer.fit_transform(self.data['text'])\n",
    "            return tfidf_title_features,tfidf_title_vectorizer\n",
    "        \n",
    "        elif self.model == 'avg':\n",
    "            w2vec_title_features = []\n",
    "            #building vector for each title \n",
    "            for i in self.data['text']:\n",
    "                w2vec_title_features.append(avg_word_vec(i,300))\n",
    "\n",
    "            #w2v_title_features = np.array(# number of doc/rows in courpus * 300) \n",
    "            Word2Vec_features = np.array(w2vec_title_features)\n",
    "\n",
    "            #saving dataframe in a npz file\n",
    "            savez_compressed(\"Pickle/Word2Vec_avg.npz\",Word2Vec_features)\n",
    "            \n",
    "            return Word2Vec_features\n",
    "        \n",
    "    def distance_similarity(self):\n",
    "        #data : data contaning text for vectorization \n",
    "        #model : method used for text vectorization\n",
    "        #Cut_off : the number of recommendations we give out\n",
    "        #df :  data set used to retrieve orignal movie description and genre\n",
    "        \n",
    "        if self.model == 'bag_of_words':  \n",
    "            title_features,title_vectorizer = self.Vectorization()\n",
    "\n",
    "            #doc_id is id on the new index formed after CountVectorizer is applied to the data['title']\n",
    "            #pairwise distances saves the distance between given input product and all other products\n",
    "            pairwise_dist = pairwise_distances(title_features,title_features[self.doc_id],metric = 'cosine')\n",
    "\n",
    "            #np.argsort returns indices of the smallest distances\n",
    "            indices = np.argsort(pairwise_dist.flatten())[:self.cut_off]\n",
    "\n",
    "            #get the index id of product in the original dataframe\n",
    "            data_indices = list(self.data.index[indices])\n",
    "\n",
    "            for i in range(0,len(data_indices)):\n",
    "                print('The Netflix movie title is {}\\n\\n'.format(self.data['title'].loc[data_indices[i]]))\n",
    "                print('The movie description is: \\n{}\\n\\n'.format(self.df['description'].loc[data_indices[i]]))\n",
    "                print('The movie is listed under:\\n{}\\n\\n'.format(self.df['listed_in'].loc[data_indices[i]]))\n",
    "                visualization(indices[i],self.data['text'].loc[data_indices[0]],self.data['text'].loc[data_indices[i]],\n",
    "                              'bag_of_words',tfidf_title_vectorizer=0,tfidf_title_features=0)\n",
    "\n",
    "        elif self.model == 'Tfidf':\n",
    "            #storing array after vectorization\n",
    "            tfidf_title_features,tfidf_title_vectorizer = self.Vectorization()\n",
    "\n",
    "            #doc_id is the id in the new index formed after CountVectorizer is applied to the data['title']\n",
    "            #pairwise distance saves the distance between given input product and all other products\n",
    "            pairwise_dist = pairwise_distances(tfidf_title_features,tfidf_title_features[self.doc_id],metric = 'cosine')\n",
    "\n",
    "            #np.argsort returns indices of the smallest distances\n",
    "            indices = np.argsort(pairwise_dist.flatten())[:self.cut_off]\n",
    "\n",
    "            #get the index id of product in the original dataframe\n",
    "            data_indices = list(self.data.index[indices])\n",
    "\n",
    "            for i in range(0,len(data_indices)):\n",
    "                visualization(indices[i], self.data['text'].loc[data_indices[0]],self.data['text'].loc[data_indices[i]],\n",
    "                              'Tfidf',tfidf_title_vectorizer,tfidf_title_features)\n",
    "                \n",
    "                print('The Netflix movie title is {}\\n\\n'.format(self.data['title'].loc[data_indices[i]]))\n",
    "                print('The movie description is: \\n{}\\n\\n'.format(self.df['description'].loc[data_indices[i]]))\n",
    "                print('The movie is listed under: \\n{}\\n\\n'.format(self.df['listed_in'].loc[data_indices[i]]))\n",
    "\n",
    "        elif self.model == 'avg':\n",
    "            #Word2Vec_features = Vectorization(data['title'],'avg')\n",
    "            #do not use vectorizer as it is computationally expensive to vectorize everytime \n",
    "            #Load the stored vectorized array .npz\n",
    "            Word2Vec_features = load(\"Pickle/Word2Vec_avg.npz\") \n",
    "\n",
    "            #uncompresing npz to numpy array array\n",
    "            Word2Vec_features  = Word2Vec_features['arr_0']\n",
    "\n",
    "            #doc_id is the id of the product in the new index formed after CountVectorizer is applied to the data['title']\n",
    "            #pairwise distance will save the distance between given input product and all other products\n",
    "            pairwise_dist = pairwise_distances(Word2Vec_features,Word2Vec_features[self.doc_id].reshape(1,-1))\n",
    "\n",
    "            #np.argsort will return indices of the smallest distances\n",
    "            indices = np.argsort(pairwise_dist.flatten())[:self.cut_off]\n",
    "\n",
    "            #get the index id of product in the original dataframe\n",
    "            data_indices = list(self.data.index[indices])\n",
    "\n",
    "            for i in range(0,len(data_indices)):\n",
    "                print('The Netflix movie title is {}\\n\\n'.format(self.data['title'].loc[data_indices[i]]))\n",
    "                print('The movie description : \\n{}\\n\\n'.format(self.df['description'].loc[data_indices[i]]))\n",
    "                print('The movie is listed under: \\n{}\\n\\n'.format(self.df['listed_in'].loc[data_indices[i]]))\n",
    "                results_Word2Vec(self.data['text'].loc[data_indices[0]],self.data['text'].loc[data_indices[i]],\n",
    "                                 indices[0],indices[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d8801",
   "metadata": {},
   "source": [
    "# Additional function needed for Word2Vec Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not vectorizer as it is computationally expensive to vectorize everytime \n",
    "#Use stored vectorized array .npz'''\n",
    "#Use this function incase we use Word2Vecvectorization \n",
    "'''\n",
    "#define Functions for AVG Word2Vec vectorization\n",
    "#We perform Word2Vec vectorization in advance and store the vectorized array to be used directly in distance \n",
    "#(continued) based similarity recommendation \n",
    "#as Word2Vec vectorization in computationally intensive as compared bag of words and tfidf vectorization.\n",
    "\n",
    "def avg_word_vec(sentence,no_features):\n",
    "    \n",
    "    # sentence: title of the apparel\n",
    "    # num_features: the lenght of word2vec vector, its values = 300\n",
    "    # model_name: model information\n",
    "    \n",
    "    featureVec = np.zeros(shape=(300,), dtype=\"float32\")\n",
    "    # intialize a vector of size 300 with all zeros\n",
    "    # add each word2vec(wordi) to this fetureVec\n",
    "\n",
    "    ncount = 0\n",
    "    for word in sentence.split():\n",
    "        ncount += 1\n",
    "        if word in vocab:\n",
    "                featureVec = np.add(featureVec,modl[word])\n",
    "            \n",
    "        if (ncount>0):\n",
    "            featureVec = np.divide(featureVec,ncount)\n",
    "\n",
    "    #return avg vec\n",
    "    return featureVec    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
