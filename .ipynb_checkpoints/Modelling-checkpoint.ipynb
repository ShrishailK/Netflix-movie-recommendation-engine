{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8b4191",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60404fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary pacakages\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import scipy.sparse\n",
    "from numpy import savez_compressed\n",
    "from numpy import load\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881074ee",
   "metadata": {},
   "source": [
    "# Result visualization for IDF,TF-IDF, Bag of words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657dc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_image(keys,values,labels,text):\n",
    "    #keys gives the list of words for recommended title\n",
    "    #divide the figure into two parts\n",
    "    \n",
    "    #plotinng a heatmap that represents the most commonly occuring words\n",
    "    plt.figure(figsize=(30,10))\n",
    "    \n",
    "    #plotinng a heatmap that represents the most commonly occuring words\n",
    "    ax = sns.heatmap(np.array([values]),annot = np.array([labels]))\n",
    "    ax.set_xticklabels(keys)\n",
    "    ax.set_title(text,fontsize=18)\n",
    "    ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 18,rotation=45)\n",
    "    \n",
    "    #displays figure\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def heatmap_image_plot(doc_id,vec1,vec2,text,model,tfidf_title_vectorizer,tfidf_title_features,idf_title_vectorizer,idf_title_features):\n",
    "    \n",
    "                     \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    \n",
    "    #set the value of non intersecting word to zero in vec2                 \n",
    "    for i in vec2.keys():\n",
    "        if i not in intersection:\n",
    "            vec2[i]=0\n",
    "    #if ith word in intersection(list of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0                 \n",
    "    values = [vec2[x] for x in vec2.keys()]\n",
    "    \n",
    "    #labels for heatmap\n",
    "    keys = list(vec2.keys())\n",
    "                     \n",
    "    if model == 'bag_of_words':\n",
    "        labels = values\n",
    "    \n",
    "    elif model == 'Tfidf':\n",
    "        labels = []\n",
    "        for i in vec2.keys():\n",
    "            if i in tfidf_title_vectorizer.vocabulary_:\n",
    "                #idf_title_vectorizer.vocabulary contains all the words in the corpus         \n",
    "                labels.append(tfidf_title_features[doc_id,tfidf_title_vectorizer.vocabulary_[i]])\n",
    "        \n",
    "            else:\n",
    "                labels.append(0)\n",
    "    elif model == 'idf':\n",
    "        labels = []\n",
    "        for i in vec2.keys():\n",
    "            if i in idf_title_vectorizer.vocabulary_:\n",
    "                #idf_title_vectorizer.vocabulary contains all the words in the corpus         \n",
    "                labels.append(idf_title_features[doc_id,idf_title_vectorizer.vocabulary_[i]])\n",
    "        \n",
    "            else:\n",
    "                labels.append(0)\n",
    "                     \n",
    "    heatmap_image(keys,values,labels,text)\n",
    "                     \n",
    "                     \n",
    "def text_vector(sentence):\n",
    "    words = sentence.split()    \n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "def results(doc_id,sentence1,sentence2,model,tfidf_title_vectorizer,tfidf_title_features,idf_title_vectorizer,idf_title_features):\n",
    "    vec1 = text_vector(sentence1)\n",
    "    vec2 = text_vector(sentence2)\n",
    "                     \n",
    "    heatmap_image_plot(doc_id,vec1,vec2,sentence2,model,tfidf_title_vectorizer,tfidf_title_features,idf_title_vectorizer,idf_title_features)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20e712",
   "metadata": {},
   "source": [
    "# Result visualization for Avg Word2Vec amd Weighted Word2Vec Vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d59561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function to better visualize and understand results\n",
    "\n",
    "def get_word_vec(sentence,doc_id):\n",
    "    #doc_id : index id in vectorized array\n",
    "    #sentence : title of product\n",
    "    \n",
    "    vec = []\n",
    "    for i in sentence.split():\n",
    "        if i in vocab:\n",
    "                vec.append(modl[i])\n",
    "        else:\n",
    "            vec.append(np.zeros(shape=(300,)))\n",
    "    return np.array(vec)\n",
    "def get_distance(vec1,vec2):\n",
    "    # vec1 = np.array(#number_of_words_title1 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    # vec2 = np.array(#number_of_words_title2 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    final_dist = []\n",
    "    for i in vec1:\n",
    "        dist = []\n",
    "        for j in vec2:\n",
    "            dist.append(np.linalg.norm(i-j))\n",
    "        final_dist.append(np.array(dist))\n",
    "            \n",
    "    return np.array(final_dist)\n",
    "\n",
    "def results_Word2Vec(sentence1,sentence2,doc_id1,doc_id2):\n",
    "    # sentence1 : title1, input product\n",
    "    # sentence2 : title2, recommended product\n",
    "   \n",
    "    sentence_vec1 = get_word_vec(sentence1,doc_id1)\n",
    "    sentence_vec2 = get_word_vec(sentence2,doc_id2)\n",
    "    \n",
    "    #sent1_sent2_dist = eucledian distance between i and j\n",
    "    #sent1_sent2_dist = np array with dimensions(#number of words in title1 * #number of words in title2)\n",
    "    sent1_sent2_dist = get_distance(sentence_vec1,sentence_vec2)\n",
    "    \n",
    "    # devide whole figure into 2 parts 1st part displays heatmap 2nd part displays image of products\n",
    "    \n",
    "    ax = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    ax = sns.heatmap(np.round(sent1_sent2_dist,3), annot = True)\n",
    "    ax.set_xticklabels(sentence2.split())\n",
    "    # set the y axis labels as input apparels title\n",
    "    ax.set_yticklabels(sentence1.split())\n",
    "    # set title as recommended apparels title\n",
    "    ax.set_title(sentence2)\n",
    "    \n",
    "    #setting the fontsize and rotation of x tick tables\n",
    "    ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 12,rotation=90)\n",
    "    ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 12,rotation=45)\n",
    "    \n",
    "    #display combine figure\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd4180",
   "metadata": {},
   "source": [
    "# Functions for Vectorization :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e83c7",
   "metadata": {},
   "source": [
    "#### Function for IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793a134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions for IDF\n",
    "def containing(word,df):\n",
    "    #returns the number of documents which have the word\n",
    "    return sum(1 for sentence in df['text'] if word in sentence.split())\n",
    "def idf(word,df):\n",
    "    #return the idf value for a word\n",
    "    return math.log(df.shape[0]/(containing(word,df)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17ff1d",
   "metadata": {},
   "source": [
    "####  Function for Word2Vec vectorization\n",
    "We perform Word2Vec vectorization in advance to use the vectorized array directly in distance based similarity recommendation as Word2Vec vectorization in computationally intensive as compared bag of words and tfidf vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e835cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_vec(sentence,no_features):\n",
    "    \n",
    "    # sentence: title of the apparel\n",
    "    # num_features: the lenght of word2vec vector, its values = 300\n",
    "    # model_name: model information\n",
    "    \n",
    "    featureVec = np.zeros(shape=(300,), dtype=\"float32\")\n",
    "    # intialize a vector of size 300 with all zeros\n",
    "    # add each word2vec(wordi) to this fetureVec\n",
    "\n",
    "    ncount = 0\n",
    "    for word in sentence.split():\n",
    "        ncount += 1\n",
    "        if word in vocab:\n",
    "                featureVec = np.add(featureVec,modl[word])\n",
    "            \n",
    "        if (ncount>0):\n",
    "            featureVec = np.divide(featureVec,ncount)\n",
    "\n",
    "    #return avg vec\n",
    "    return featureVec    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d564f5",
   "metadata": {},
   "source": [
    "#### Downloading the google Word2Vec library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "129e3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading Googles Word2Vec library to be used in all word to vec models\n",
    "#using a pretrained model by google\n",
    "#download \"GoogleNews-vectors-negative300.bin\" \n",
    "\n",
    "modl = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#vocab = stores all the words in google Word2vec model\n",
    "vocab = modl.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069eaa43",
   "metadata": {},
   "source": [
    "# Vectorization : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorization(data,model):\n",
    "    #data : Data set containing text data\n",
    "    #model : method used for text vectorization\n",
    "    \n",
    "    if model == 'bag_of_words':\n",
    "        #Vectorization using Bag of words\n",
    "        title_vectorizer = CountVectorizer()\n",
    "        title_features = title_vectorizer.fit_transform(data['text'])\n",
    "        \n",
    "        return title_features,title_vectorizer\n",
    "    \n",
    "    elif model == 'Tfidf':\n",
    "        #Vectorization using tfidfVectorizer\n",
    "        tfidf_title_vectorizer = TfidfVectorizer()\n",
    "        tfidf_title_features = tfidf_title_vectorizer.fit_transform(data['text'])\n",
    "        \n",
    "        return tfidf_title_features,tfidf_title_vectorizer\n",
    "    \n",
    "    elif model == 'avg':\n",
    "        w2vec_title_features = []\n",
    "        #building vector for each title \n",
    "        for i in data['text']:\n",
    "            w2vec_title_features.append(avg_word_vec(i,300))\n",
    "\n",
    "        #w2v_title_features = np.array(# number of doc/rows in courpus * 300) \n",
    "        Word2Vec_features = np.array(w2vec_title_features)\n",
    "        \n",
    "        #saving dataframe in a npz file\n",
    "        savez_compressed(\"Pickle/Word2Vec_avg.npz\",Word2Vec_features)\n",
    "        \n",
    "        return w2vec_title_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4e0fa",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_similarity(doc_id,data,model,cut_off):\n",
    "    #data : data contaning text for vectorization \n",
    "    #model : method used for text vectorization\n",
    "    #Cut_off : the number of recommendations we give out\n",
    "    \n",
    "    if model == 'bag_of_words': \n",
    "        #storing array after vectorization \n",
    "        title_features,title_vectorizer = Vectorization(data,model)\n",
    "\n",
    "        #doc_id is id on the new index formed after CountVectorizer is applied to the data['title']\n",
    "        #pairwise distances saves the distance between given input product and all other products\n",
    "        pairwise_dist = pairwise_distances(title_features,title_features[doc_id],metric = 'cosine')\n",
    "\n",
    "        #np.argsort returns indices of the smallest distances\n",
    "        indices = np.argsort(pairwise_dist.flatten())[:cut_off]\n",
    "\n",
    "        #get the index id of product in the original dataframe\n",
    "        data_indices = list(data.index[indices])\n",
    "\n",
    "        for i in range(0,len(data_indices)):\n",
    "            results(indices[i], data['text'].loc[data_indices[0]], data['text'].loc[data_indices[i]],model,tfidf_title_vectorizer=0,tfidf_title_features=0,idf_title_vectorizer=0,idf_title_features=0)\n",
    "            print('The Netflix movie title is {}'.format(data['title'].loc[data_indices[i]]))\n",
    "            \n",
    "    elif model == 'Tfidf':\n",
    "        #storing array after vectorization \n",
    "        tfidf_title_features,tfidf_title_vectorizer = Vectorization(data,model)\n",
    "\n",
    "        #doc_id is the id in the new index formed after CountVectorizer is applied to the data['title']\n",
    "        #pairwise distance saves the distance between given input product and all other products\n",
    "        pairwise_dist = pairwise_distances(tfidf_title_features,tfidf_title_features[doc_id],metric = 'cosine')\n",
    "\n",
    "        #np.argsort returns indices of the smallest distances\n",
    "        indices = np.argsort(pairwise_dist.flatten())[:cut_off]\n",
    "\n",
    "        #get the index id of product in the original dataframe\n",
    "        data_indices = list(data.index[indices])\n",
    "\n",
    "        for i in range(0,len(data_indices)):\n",
    "            results(indices[i], data['text'].loc[data_indices[0]], data['text'].loc[data_indices[i]],model,tfidf_title_vectorizer=tfidf_title_vectorizer,tfidf_title_features=tfidf_title_features,idf_title_vectorizer=0,idf_title_features=0)\n",
    "            print('The Netflix movie title is {}'.format(data['title'].loc[data_indices[i]]))\n",
    "        \n",
    "    elif model == 'idf':\n",
    "        #do not use vectorizer as it is computationally expensive to vectorize everytime\n",
    "        #Load the saved vectorized sparse array .npz\n",
    "        #title_features= Vectorization(data,'idf')\n",
    "        idf_title_features = scipy.sparse.load_npz('Pickle/idf_title_features.npz') #OR we can Vectorize using the code above\n",
    "        \n",
    "        #to get the words in columns implement count vectorizers\n",
    "        idf_title_vectorizer = CountVectorizer()\n",
    "        vectorizer = idf_title_vectorizer.fit_transform(data['text'])\n",
    "\n",
    "        #doc_id is the id in the new index formed after CountVectorizer is applied to the data['title']\n",
    "        #pairwise distance will save the distance between given input product and all other products\n",
    "        pairwise_dist = pairwise_distances(idf_title_features,idf_title_features[doc_id],metric = 'cosine')\n",
    "\n",
    "        #np.argsort will return indices of the smallest distances\n",
    "        indices = np.argsort(pairwise_dist.flatten())[:cut_off]\n",
    "\n",
    "        #get the index id of product in the original dataframe\n",
    "        data_indices = list(data.index[indices])\n",
    "\n",
    "        for i in range(0,len(data_indices)):\n",
    "            results(indices[i], data['text'].loc[data_indices[0]], data['text'].loc[data_indices[i]], model, tfidf_title_vectorizer=0, tfidf_title_features=0, idf_title_vectorizer = idf_title_vectorizer, idf_title_features = idf_title_features)\n",
    "            print('The Netflix movie title is {}'.format(data['title'].loc[data_indices[i]]))\n",
    "    \n",
    "    \n",
    "    elif model == 'avg':\n",
    "        #Word2Vec_features = Vectorization(data['title'],'avg')\n",
    "        #do not use vectorizer as it is computationally expensive to vectorize everytime \n",
    "        #Load the stored vectorized array .npz\n",
    "        Word2Vec_features = load(\"Pickle/Word2Vec_avg.npz\") \n",
    "        \n",
    "        #uncompresing npz to numpy array array\n",
    "        Word2Vec_features  = Word2Vec_features['arr_0']\n",
    "        \n",
    "        #doc_id is the id of the product in the new index formed after CountVectorizer is applied to the data['title']\n",
    "        #pairwise distance will save the distance between given input product and all other products\n",
    "        pairwise_dist = pairwise_distances(Word2Vec_features,Word2Vec_features[doc_id].reshape(1,-1))\n",
    "\n",
    "        #np.argsort will return indices of the smallest distances\n",
    "        indices = np.argsort(pairwise_dist.flatten())[:cut_off]\n",
    "\n",
    "        #get the index id of product in the original dataframe\n",
    "        data_indices = list(data.index[indices])\n",
    "\n",
    "        for i in range(0,len(data_indices)):\n",
    "            results_Word2Vec(data['text'].loc[data_indices[0]], data['text'].loc[data_indices[i]], indices[0], indices[i])\n",
    "            print('The Netflix movie title is {}'.format(data['title'].loc[data_indices[i]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
