{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33201082",
   "metadata": {},
   "source": [
    "# Importing Libraries and Storing Googles Word2vec libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6329f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "#Downloading Googles Word2Vec library to be used in all word to vec models using a pretrained model by google\n",
    "#download \"GoogleNews-vectors-negative300.bin\" \n",
    "modl = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#vocab = stores all the words in google Word2vec model\n",
    "vocab = modl.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2de43",
   "metadata": {},
   "source": [
    "# Result Visualization\n",
    "#### TF-IDF, Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa388291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_image(keys,values,labels,text):\n",
    "    #keys gives the list of words for recommended title\n",
    "    #divide the figure into two parts\n",
    "\n",
    "    #plotinng a heatmap that represents the most commonly occuring words\n",
    "    plt.figure(figsize=(30,10))\n",
    "\n",
    "    #plotinng a heatmap that represents the most commonly occuring words\n",
    "    ax = sns.heatmap(np.array([values]),annot = np.array([labels]))\n",
    "    ax.set_xticklabels(keys)\n",
    "    ax.set_title(text,fontsize=18)\n",
    "    ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 18,rotation=45)\n",
    "\n",
    "    #displays figure\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def heatmap_image_plot(doc_id,vec1,vec2,text,model,tfidf_title_vectorizer,tfidf_title_features):\n",
    "\n",
    "\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "\n",
    "    #set the value of non intersecting word to zero in vec2                 \n",
    "    for i in vec2.keys():\n",
    "        if i not in intersection:\n",
    "            vec2[i]=0\n",
    "    #if ith word in intersection(list of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0                 \n",
    "    values = [vec2[x] for x in vec2.keys()]\n",
    "\n",
    "    #labels for heatmap\n",
    "    keys = list(vec2.keys())\n",
    "\n",
    "    if model == 'bag_of_words':\n",
    "        labels = values\n",
    "\n",
    "    elif model == 'Tfidf':\n",
    "        labels = []\n",
    "        for i in vec2.keys():\n",
    "            if i in tfidf_title_vectorizer.vocabulary_:\n",
    "                #idf_title_vectorizer.vocabulary contains all the words in the corpus         \n",
    "                labels.append(tfidf_title_features[doc_id,tfidf_title_vectorizer.vocabulary_[i]])\n",
    "\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    \n",
    "    heatmap_image(keys,values,labels,text)\n",
    "\n",
    "\n",
    "def text_vector(sentence):\n",
    "    words = sentence.split()    \n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "def visualization(doc_id,sentence1,sentence2,model,tfidf_title_vectorizer,tfidf_title_features):\n",
    "    vec1 = text_vector(sentence1)\n",
    "    vec2 = text_vector(sentence2)\n",
    "\n",
    "    heatmap_image_plot(doc_id,vec1,vec2,sentence2,model,tfidf_title_vectorizer,tfidf_title_features)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e99aba",
   "metadata": {},
   "source": [
    "#### Avg Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acdd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vec(sentence,doc_id):\n",
    "    #doc_id : index id in vectorized array\n",
    "    #sentence : title of product\n",
    "    \n",
    "    vec = []\n",
    "    for i in sentence.split():\n",
    "        if i in vocab:\n",
    "                vec.append(modl[i])\n",
    "        else:\n",
    "            vec.append(np.zeros(shape=(300,)))\n",
    "    return np.array(vec)\n",
    "\n",
    "def get_distance(vec1,vec2):\n",
    "    # vec1 = np.array(#number_of_words_title1 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    # vec2 = np.array(#number_of_words_title2 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    final_dist = []\n",
    "    for i in vec1:\n",
    "        dist = []\n",
    "        for j in vec2:\n",
    "            dist.append(np.linalg.norm(i-j))\n",
    "        final_dist.append(np.array(dist))\n",
    "            \n",
    "    return np.array(final_dist)\n",
    "\n",
    "def results_Word2Vec(sentence1,sentence2,doc_id1,doc_id2):\n",
    "    # sentence1 : title1, input product\n",
    "    # sentence2 : title2, recommended product\n",
    "   \n",
    "    sentence_vec1 = get_word_vec(sentence1,doc_id1)\n",
    "    sentence_vec2 = get_word_vec(sentence2,doc_id2)\n",
    "    \n",
    "    #sent1_sent2_dist = eucledian distance between i and j\n",
    "    #sent1_sent2_dist = np array with dimensions(#number of words in title1 * #number of words in title2)\n",
    "    sent1_sent2_dist = get_distance(sentence_vec1,sentence_vec2)\n",
    "    \n",
    "    # devide whole figure into 2 parts 1st part displays heatmap 2nd part displays image of products\n",
    "    \n",
    "    ax = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    ax = sns.heatmap(np.round(sent1_sent2_dist,3), annot = True)\n",
    "    ax.set_xticklabels(sentence2.split())\n",
    "    # set the y axis labels as input apparels title\n",
    "    ax.set_yticklabels(sentence1.split())\n",
    "    # set title as recommended apparels title\n",
    "    ax.set_title(sentence2)\n",
    "    \n",
    "    #setting the fontsize and rotation of x tick tables\n",
    "    ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 12,rotation=90)\n",
    "    ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 12,rotation=45)\n",
    "    \n",
    "    #display combine figure\n",
    "    plt.show()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
